{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Natural language Processing\n",
    "In this notebook, we'll start using Python's RegEx package and do a simple natural language processing exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Strings?\n",
    "Strings are a type of data usually used to store text. They are different from other data types in python like integers, lists, or dictionaries. In python, they are usually enclosed by parantheses, either \"\" or ''. When checking an object's type, if it is a string, you'll see `str`. Integers will display `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check this object's type\n",
    "type(\"hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check this object's type\n",
    "type(342)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strings can also be combined into objects like lists and dictionaries. Lists are just that, a combination of objects like a bunch of strings or a bunch of numbers. They are enclosed in brackets [ ]. Dictionaries are kind of like lists, but instead of string information like this:\n",
    "\n",
    "* Capucchino\n",
    "* Latte\n",
    "* Chai\n",
    "\n",
    "it can store pairs of objects linked together, kind of like this:\n",
    "\n",
    "Key | Value\n",
    "--- | ---\n",
    "one: | you're like a dream come true\n",
    "two: | just wanna be with you\n",
    "three: | you know it's plain to see, that you're the only one for me\n",
    "\n",
    "Dictionaries are inside of curly brackets { }."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type([\"Capucchino\", \"Latte\", \"Chai\"]))\n",
    "print(type({\"one\":\"you're like a dream come true\", \"two\": \"just wanna be with you\", \"three\": \"you know it's plain to see, that you're the only one for me\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's RegEx?\n",
    "Regular expressions, or RegEx are a way to parse strings to make them more useful to a computer program. For example, we might want to separate a paragraph into separate sentences to assess average sentence length, or parse a text into separate words to do a word frequency analysis or remove unwanted common words like \"to\" and \"the\" that don't give us the information we want.\n",
    "\n",
    "We can do things like match a substring, or search for a substring within another string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 3), match='You'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import regex\n",
    "import re\n",
    "re.match(\"You\", \"You know nothing John Snow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 6), match='Winter'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"Winter\", \"Winter is coming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between `match` and `search` is that `match` will see if the two strings match *from the very beginning,* whereas the search will search for the first string anywhere within the second string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match(\"know\", \"You know nothing John Snow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(7, 9), match='is'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"is\", \"Winter is coming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegEx Patterns\n",
    "RegEx also allows us to perform operations on certain classes of strings. for example, '\\w+' searches for words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 2), match='Is'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = '\\w+'\n",
    "re.search(word, \"Is it too late now to say sorry?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some common RegEx patterns:\n",
    "    \n",
    "pattern | matches | example\n",
    "--- | --- | ---\n",
    "\\w+ | word | \"Bieber\"\n",
    "\\d | digits | 250624\n",
    "\\s | spaces | ' '\n",
    ".\\* | matches anything | 25or6to4\n",
    "+ or \\* | allows matching to the end of a string or pattern | 'Aaaaaaaaaaaa'\n",
    "\n",
    "Capitalizing something negates it.  For example \\S would return anything that is NOT a space. You can also create a range of characters with square brackets:\n",
    "\n",
    "pattern | matches | example\n",
    "--- | --- | ---\n",
    "[a-z] | lowercase group | \"sorry\"\n",
    "[A-Za-z@.] | upper, lower case letters, at sign, and period | MyEmail@bah.com\n",
    "\n",
    "Run the code below to split the string below, on spaces, into separate words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'baristas',\n",
       " 'in',\n",
       " 'this',\n",
       " 'coffee',\n",
       " 'shop',\n",
       " 'are',\n",
       " 'playing',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'Bieber.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('\\s+', \"The baristas in this coffee shop are playing a lot of Bieber.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started Processing Text\n",
    "Now we'll get started processing text with `re.split()` and `re.findall()`. Please complete and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Congrats on your first RegEx split', '  How was it', '  Hopefully not too difficult', '  Can you find 5 sentences', '']\n",
      "['Congrats', 'RegEx', 'How', 'Hopefully', 'Can']\n",
      "['Congrats', 'on', 'your', 'first', 'RegEx', 'split!', 'How', 'was', 'it?', 'Hopefully', 'not', 'too', 'difficult.', 'Can', 'you', 'find', '5', 'sentences?']\n",
      "['5']\n"
     ]
    }
   ],
   "source": [
    "first_string = \"Congrats on your first RegEx split!  How was it?  Hopefully not too difficult.  Can you find 5 sentences?\"\n",
    "\n",
    "# TO DO: Write a regular expression to identify the sentence endings \"!, ?, and .\". \n",
    "## To do this, first type \"r\" to make sure the period registers as a period\n",
    "## instead of being read as the \"match anything\" symbol. Right after the r, with\n",
    "## no spaces, type your brackets, and put the three sentence end symbols inside\n",
    "## the brackets with no spaces or commas.\n",
    "sentence_endings = r\"[!?.]\"\n",
    "\n",
    "# Next, split the string \"first_string\" into sentences based on the sentence_endings \n",
    "## expression you just created, and print the result. The split function will take\n",
    "## 2 inputs inside the inner parentheses, and these two inputs will be separated by \n",
    "## the comma. The first object will be the name of the regular expression you created above\n",
    "## and the second input will be my_string.\n",
    "print(re.split(sentence_endings, first_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result. First create the range of \n",
    "## capital characters inside the brackets, then follow it immediately, no spaces, with\n",
    "## the RegEx pattern for words, all inside the quotation marks.\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, first_string))\n",
    "\n",
    "# Now split my_string on spaces and print the result. Remember the spaces RegEx pattern and put it in the quotes\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, first_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d\"\n",
    "print(re.findall(digits, first_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing a File and Tokenization\n",
    "Now we're going to import a text file, the first chapter of Pride and Prejudice, and parse the chapter into individual sentences and words. This parsing is called \"tokenization\" because it returns an array of the individual words or sentences, and we can conduct future operations and analysis on those arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sonjalindberg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "<class 'str'>\n",
      "['cried', 'his', 'wife', 'impatiently.\\\\r', \"''\", 'You', 'want', 'to', 'tell', 'me', ',', 'and', 'I', 'have', 'no', 'objection', 'to', 'hearing', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "# Import packages. Don't worry aobut these too much right now.\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# TO DO: Import sent_tokenize from nltk.tokenize and then import word_tokenize from nltk.tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# TO DO: Import text file of Pride and Prejudice, chapter one by typing the file name \n",
    "## \"pride_and_prejudice.txt\" inside the first set of parentheses.\n",
    "chapter_one = str(open(\"pride_and_prejudice.txt\", \"rb\").read())\n",
    "\n",
    "print(type(chapter_one))\n",
    "\n",
    "# TO DO: Split chapter_one into sentences by typing chapter_one inside of the parantheses\n",
    "sentences = sent_tokenize(chapter_one)\n",
    "\n",
    "# TO DO: Use word_tokenize to tokenize the fourth sentence by putting a 3 inside the brackets, \n",
    "## and print. Note: many computer programs count from 0, not 1.  Therefore, the first sentence \n",
    "## would be indexed 0, the second would be indexed 1, etc.\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "print(tokenized_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'few', 'preference', 'it.', 'mind', 'sure', 'quick', 'glad', 'parts', 'want', 'Depend', 'How', 'assure', 'daughters.\\\\r', 'them', 'life', 'Lady', '?', 'children', 'acknowledged', 'all', 'flatter', 'its', 'extraordinary', 'develope', 'fall', 'down', 'will', 'be', 'The', 'truth', 'sarcastic', 'mention', 'whichever', 'come', 'design', 'she', 'news.\\\\r\\\\r', 'throw', 'came', 'much', 'party', 'four', 'thinking', 'day', 'ought', 'in', 'uncertain', 'Mrs.', 'enough.\\\\r', 'nonsense', 'It', 'Oh', 'large', 'certainly', 'three-and-twenty', 'marrying', 'but', 'send', 'desire', 'half', 'answer.\\\\r', 'fine', 'solace', 'end', 'had', 'Netherfield', 'us', 'establishment', 'Bingley', 'see', 'live', '!', 'England', 'universally', 'about', 'love', 'place', 'of', 'made', 'may', 'said', 'last', 'been', 'cases', 'over-scrupulous', 'think', 'herself', 'from', 'something', 'experience', 'go', 'What', 'men', '\\\\rThis', 'on', 'least.', 'might', 'like', 'Park', 'sisters.', 'that', 'delight', 'They', 'hope', 'Design', 'mixture', 'servants', 'account', 'when', 'William', \"''\", 'vexing', 'But', 'are', ';', 'handsome', 'visiting', 'respect', 'families', 'better', 'to', 'possession', 'Bennet', 'bit', 'pretend', \"'\", 'they', 'than', 'When', 'Indeed', 'upon', 'perhaps', 'can', 'talk', 'merely', 'what', 'next', 'A', 'as', 'giving', 'many', 'it', 'let', 'best', 'an', 'likely', 'girls', 'make', 'She', 'would', 'do', 'fortune', 'objection', 'our', 'reserve', 'though', '.', 'over', 'Only', 'you', 'dare', 'is', 'use', 'always', 'must', 'was', 'abuse', 'little', 'since', 'poor', 'name', 'Is', 'years', 'minds', 'returned', 'You', 'my', 'says', 'I', 'me', 'how', 'surrounding', 'Lydia', 'compassion', 'My', 'here', 'am', 'hearty', 'comes', 'soon', 'tiresome', 'Mr.', 'if', 'discontented', 'Why', 'high', 'humour', 'impatiently.\\\\r', 'well', 'mistake', 'so', 'very', '\\\\rMr', 'Sir', 'his', 'the', 'rightful', 'week', 'settling', 'know', 'feelings', 'new-comers', \"b'It\", 'hearing', 'general', 'word', 'ignorant', ',', 'others', 'consider', 'neighbourhood', 'themselves', 'quickness', 'woman', 'Mr', 'nerves', 'share', 'affect', 'daughters', 'thousand', 'wife.\\\\rHowever', 'consideration', 'none', 'before', 'heard', 'him', 'agreed', 'some', 'less', 'who', 'surely', 'thing', 'five', 'nervous', 'way', 'insufficient', 'lines', 'nor', 'Monday', 'her', 'indeed', 'house', 'temper', 'Jane', 'year', 'odd', 'determined', 'good-humoured', 'considered', 'by', 'Lizzy', ':', 'any', 'still', 'which', 'engage', 'Morris', 'single', 'Ah', 'suffer', 'consent', 'occasion', 'north', 'character', 'lady', 'recommend', 'told', 'there', 'silly', 'mean', 'Her', 'Michaelmas', 'should', 'property', 'business', 'one', 'their', 'young', 'beauty', 'dear', 'first', 'a', 'other', 'Lucas', 'fixed', 'get', 'tell', 'married', 'say', 'with', 'your', 'has', 'delighted', 'and', 'grown-up', '``', 'wife', 'chaise', 'Do', 'fancied', 'impossible', 'such', 'for', 'understanding', 'give', 'twenty', 'cried', 'good', 'no', 'not.\\\\r', 'chuses', 'views', 'take', 'taken', 'often', 'friends', 'this', 'caprice', 'or', 'difficult', 'understand', 'at', 'invitation', 'therefore', 'he', 'entering', 'not', 'man', 'Long', 'visit', 'just', 'replied', 'all.', 'these', 'information', 'known', 'now', 'immediately', '\\\\r', 'have', 'into', 'In', 'more', 'old', 'own'}\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Make a set of unique tokens in the entire scene by putting chapter_one inside the parentheses.\n",
    "unique_tokens = set(word_tokenize(chapter_one))\n",
    "\n",
    "# TO DO: Print the unique tokens result by typing unique_tokens inside the parentheses.\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Things in Text\n",
    "Nice work! You've already turned raw text into something more useful and easier to work with: a list of all the unique words in Chapter 1 of the book. You'll notice capitalized words are counted as a completely different word from lowercase words, and that punctuation is also counted as its own word. This could make analysis difficult, since we usually want to count forms of the same word together regardless of capitalization, and since in novels we usually don't care about symbols and punctuation (although in tweets, we'd care aobut symbols like @ and \\#). Now let's learn how to search within a text. We'll find the location of the first mention of servants, and create an array of every quote in the first chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1155 1163\n",
      "['My dear Mr. Bennet,', 'have you heard that Netherfield Park is let at last?', 'But it is,', 'for Mrs. Long has just been here, and she told me all about it.', 'Do not you want to know who has taken it?', 'You want to tell me, and I have no objection to hearing it.', 'Why, my dear, you must know, Mrs. Long says that Netherfield is taken by a young man of large fortune from the north of England; that he came down on Monday in a chaise and four to see the place, and was so much delighted with it, that he agreed with Mr. Morris immediately; that he is to take possession before Michaelmas, and some of his servants are to be in the house by the end of next week.', 'What is his name?', 'Bingley.', 'Is he married or single?', 'Oh! single, my dear, to be sure! A single man of large fortune; four or five thousand a year. What a fine thing for our girls!', 'How so? how can it affect them?', 'My dear Mr. Bennet,', 'how can you be so tiresome! You must know that I am thinking of his marrying one of them.', 'Is that his design in settling here?', 'Design! nonsense, how can you talk so! But it is very likely that he may fall in love with one of them, and therefore you must visit him as soon as he comes.', 'I see no occasion for that. You and the girls may go, or you may send them by themselves, which perhaps will be still better, for as you are as handsome as any of them, Mr. Bingley might like you the best of the party.', 'My dear, you flatter me. I certainly have had my share of beauty, but I do not pretend to be any thing extraordinary now. When a woman has five grown-up daughters she ought to give over thinking of her own beauty.', 'In such cases a woman has not often much beauty to think of.', 'But, my dear, you must indeed go and see Mr. Bingley when he comes into the neighbourhood.', 'It is more than I engage for, I assure you.', 'But consider your daughters. Only think what an establishment it would be for one of them. Sir William and Lady Lucas are determined to go, merely on that account, for in general, you know, they visit no new-comers. Indeed you must go, for it will be impossible for us to visit him if you do not.', 'You are over-scrupulous surely. I dare say Mr. Bingley will be very glad to see you; and I will send a few lines by you to assure him of my hearty consent to his marrying whichever he chuses of the girls: though I must throw in a good word for my little Lizzy.', 'I desire you will do no such thing. Lizzy is not a bit better than the others; and I am sure she is not half so handsome as Jane, nor half so good-humoured as Lydia. But you are always giving her the preference.', 'They have none of them much to recommend them,', 'they are all silly and ignorant, like other girls; but Lizzy has something more of quickness than her sisters.', 'Mr. Bennet, how can you abuse your own children in such a way! You take delight in vexing me. You have no compassion on my poor nerves.', 'You mistake me, my dear. I have a high respect for your nerves. They are my old friends. I have heard you mention them with consideration these twenty years at least.', 'Ah! you do not know what I suffer.', 'But I hope you will get over it, and live to see many young men of four thousand a year come into the neighbourhood.', 'It will be no use to us if twenty such should come, since you will not visit them.', 'Depend upon it, my dear, that when there are twenty, I will visit them all.']\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Search for the first occurrence of \"servants\" in scene_one by running re.search on \"servants\"\n",
    "## and chapter_one\n",
    "match = re.search(\"servants\", chapter_one)\n",
    "\n",
    "# Print the start and end indexes of match, match.start() and match.end() respectively\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for quotes. Hint: remember the regex expression should be \n",
    "## r followed by quotation marks, with the expression you want written inside the quotation marks.\n",
    "## The \\ symbol makes sure that the symbol right after it is read literally instead of as a symbol \n",
    "## for some other action. For example, \\. is read as a period instead of as a \"match anything\" \n",
    "## character. Also, put (.+?) inside of the quotes to capture the text inside. Don't worry too much about\n",
    "## the meaning of (.+?) right now.\n",
    "pattern1 = r'\\\"(.+?)\\\"'    #this is incorrect, not sure how to represent qyoted portions\n",
    "\n",
    "# Inside the parentheses, use re.findall on pattern1 and chapter_one to return an array of every quote in Chapter 1\n",
    "print(re.findall(pattern1, chapter_one))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the most common Tokens\n",
    "You've got this! Now, back to the word tokens. We're going to split the chapter into word tokens, conver these to lower case so that all case versions of the same word are counted together, and then print the 10 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 50), (\"''\", 43), ('.', 39), ('you', 31), ('of', 29), ('\\\\r', 23), ('to', 22), ('a', 21), ('``', 21), ('the', 18)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# TO DO: word_tokenize chapter_one\n",
    "tokens = word_tokenize(chapter_one)\n",
    "\n",
    "# Convert the tokens into lowercase\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "So... that ended up not being that useful.  We can see that punctuation and filler words make up the bulk of the tokens in this chapter. Luckily, there's a way to get rid of these words, which are called **stop words**. nltk has a built in list of this, but just to give you an idea of what the list looks like, I've spelled out a stopwords list below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = (\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization and Getting Rid of Symbols and Punctuation\n",
    "We'll also import a tool called a lemmatizer.  This will change different forms of a word into the same root.  For example, all versions of \"sing\", \"sang\", and \"sung\" would become \"sing\", and both \"pineapple\" and \"pineapples\" would be counted as \"pineapple.\" Finally, we'll retain only alphabetic words to get rid of those pesky punctuation \"words\" like : and \" that were crowding our arrays of common word tokens. Finally, we'll print the 10 most common words from this new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sonjalindberg/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[('i', 17), ('dear', 8), ('bennet', 6), ('know', 5), ('visit', 5), ('single', 4), ('man', 4), ('bingley', 4), ('year', 4), ('girl', 4), ('come', 4), ('fortune', 3), ('want', 3), ('little', 3), ('neighbourhood', 3), ('replied', 3), ('wife', 3), ('thing', 3), ('beauty', 3), ('woman', 3), ('daughter', 3), ('lizzy', 3), ('truth', 2), ('possession', 2), ('good', 2), ('mind', 2), ('lady', 2), ('heard', 2), ('netherfield', 2), ('long', 2), ('taken', 2), ('say', 2), ('young', 2), ('large', 2), ('married', 2), ('sure', 2), ('thousand', 2), ('thinking', 2), ('marrying', 2), ('design', 2), ('send', 2), ('better', 2), ('handsome', 2), ('like', 2), ('think', 2), ('assure', 2), ('half', 2), ('nerve', 2), ('universally', 1), ('acknowledged', 1), ('known', 1), ('feeling', 1), ('view', 1), ('entering', 1), ('fixed', 1), ('surrounding', 1), ('family', 1), ('considered', 1), ('rightful', 1), ('property', 1), ('said', 1), ('day', 1), ('park', 1), ('let', 1), ('returned', 1), ('just', 1), ('told', 1), ('cried', 1), ('tell', 1), ('objection', 1), ('hearing', 1), ('invitation', 1), ('north', 1), ('england', 1), ('came', 1), ('monday', 1), ('chaise', 1), ('place', 1), ('delighted', 1), ('agreed', 1), ('morris', 1), ('immediately', 1), ('michaelmas', 1), ('servant', 1), ('house', 1), ('end', 1), ('week', 1), ('oh', 1), ('fine', 1), ('affect', 1), ('tiresome', 1), ('settling', 1), ('nonsense', 1), ('talk', 1), ('likely', 1), ('fall', 1), ('love', 1), ('soon', 1), ('occasion', 1), ('best', 1), ('party', 1), ('flatter', 1), ('certainly', 1), ('share', 1), ('pretend', 1), ('extraordinary', 1), ('ought', 1), ('case', 1), ('engage', 1), ('consider', 1), ('establishment', 1), ('sir', 1), ('william', 1), ('lucas', 1), ('determined', 1), ('merely', 1), ('account', 1), ('general', 1), ('impossible', 1), ('surely', 1), ('dare', 1), ('glad', 1), ('line', 1), ('hearty', 1), ('consent', 1), ('whichever', 1), ('chuses', 1), ('throw', 1), ('word', 1), ('desire', 1), ('bit', 1), ('jane', 1), ('lydia', 1), ('giving', 1), ('preference', 1), ('recommend', 1), ('silly', 1), ('ignorant', 1), ('quickness', 1), ('mr', 1), ('abuse', 1), ('child', 1), ('way', 1), ('delight', 1), ('vexing', 1), ('compassion', 1), ('poor', 1), ('mistake', 1), ('high', 1), ('respect', 1), ('old', 1), ('friend', 1), ('mention', 1), ('consideration', 1), ('ah', 1), ('suffer', 1), ('hope', 1), ('live', 1), ('men', 1), ('use', 1), ('depend', 1), ('odd', 1), ('mixture', 1), ('quick', 1), ('part', 1), ('sarcastic', 1), ('humour', 1), ('reserve', 1), ('caprice', 1), ('experience', 1), ('insufficient', 1), ('make', 1), ('understand', 1), ('character', 1), ('difficult', 1), ('develope', 1), ('mean', 1), ('understanding', 1), ('information', 1), ('uncertain', 1), ('temper', 1), ('discontented', 1), ('fancied', 1), ('nervous', 1), ('business', 1), ('life', 1), ('solace', 1), ('visiting', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [w for w in alpha_only if w not in stopwords]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# TO DO: Print the 10 most common tokens by typing 10 into the parentheses.\n",
    "print(bow.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! Notice the words \"girl\", \"come\", and \"daughter\" have been lemmatized so all forms of those words are counted together. Congrats! You've just done your first word frequency analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
